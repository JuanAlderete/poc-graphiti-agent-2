# POC: Graphiti Agent ‚Äî RAG H√≠brido con Despliegue por Fases

Esta prueba de concepto valida si la arquitectura **Graphiti + PostgreSQL/pgvector** es econ√≥micamente viable para producci√≥n. El sistema combina la velocidad de b√∫squeda vectorial de Postgres con la capacidad de razonamiento relacional de Graphiti/Neo4j, y est√° dise√±ado para activarse en etapas: arranc√°s barato con solo Postgres y activ√°s el grafo cuando el negocio lo justifica.

---

## √çndice

1. [¬øQu√© problema resuelve?](#qu√©-problema-resuelve)
2. [Arquitectura general](#arquitectura-general)
3. [Estrategia de despliegue por fases](#estrategia-de-despliegue-por-fases)
4. [Instalaci√≥n y configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
5. [C√≥mo correr el proyecto](#c√≥mo-correr-el-proyecto)
6. [Estructura de archivos explicada](#estructura-de-archivos-explicada)
7. [Flujo de datos de punta a punta](#flujo-de-datos-de-punta-a-punta)
8. [Sistema de m√©tricas y costos](#sistema-de-m√©tricas-y-costos)
9. [Optimizaciones de costo implementadas](#optimizaciones-de-costo-implementadas)
10. [Criterios de √©xito (GO / OPTIMIZE / STOP)](#criterios-de-√©xito)
11. [Preguntas frecuentes](#preguntas-frecuentes)

---

## ¬øQu√© problema resuelve?

El cliente tiene una base de conocimiento (transcripciones de podcasts, gu√≠as, playbooks) y necesita un agente que pueda responder preguntas usando esa informaci√≥n. La duda es: **¬øcu√°nto cuesta realmente operar esto a escala?**

Este POC responde esa pregunta midiendo el costo exacto (en USD) de cada operaci√≥n: ingestar un documento, hacer una b√∫squeda, generar un email. Con esos datos, el sistema proyecta el gasto mensual y anual bajo distintos escenarios.

La arquitectura tambi√©n resuelve un problema t√©cnico: ¬øc√≥mo activar un knowledge graph sin tirar todo lo que ya est√° corriendo? La respuesta es la **migraci√≥n por hidrataci√≥n** ‚Äî los documentos ya guardados en Postgres se pueden "hidratar" a Neo4j en un paso separado, sin re-ingestar archivos ni interrumpir el servicio.

---

## Arquitectura general

```
Documentos (.md)
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ingestion/ingest.py        ‚îÇ  Pipeline de ingesta
‚îÇ                              ‚îÇ
‚îÇ  1. Parsea frontmatter YAML  ‚îÇ
‚îÇ  2. Extrae entidades (gratis)‚îÇ  <- sin LLM, solo regex
‚îÇ  3. Strip Markdown           ‚îÇ  <- reduce tokens Graphiti
‚îÇ  4. Chunking por segmentos   ‚îÇ
‚îÇ  5. Genera embeddings        ‚îÇ  <- OpenAI / Gemini
‚îÇ  6. Guarda en Postgres       ‚îÇ
‚îÇ  7. (Opcional) -> Graphiti   ‚îÇ  <- truncado a 6000 chars
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                ‚îÇ
         ‚ñº                ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ PostgreSQL ‚îÇ    ‚îÇ  Neo4j    ‚îÇ
  ‚îÇ (pgvector) ‚îÇ    ‚îÇ(Graphiti) ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  agent/tools.py ‚îÇ  Capa de b√∫squeda
        ‚îÇ                 ‚îÇ
        ‚îÇ ‚Ä¢ vector_search ‚îÇ  <- cosine similarity (top 3)
        ‚îÇ ‚Ä¢ graph_search  ‚îÇ  <- relaciones/entidades (top 3)
        ‚îÇ ‚Ä¢ hybrid_search ‚îÇ  <- RRF (combina ambas) (top 3)
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ poc/content_    ‚îÇ  Generaci√≥n de contenido
        ‚îÇ generator.py    ‚îÇ  (emails, reels, historias)
        ‚îÇ                 ‚îÇ  max_tokens por formato
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Stack tecnol√≥gico:**
- **Python 3.10+** con `asyncio` / `asyncpg` para I/O no bloqueante
- **PostgreSQL** con extensi√≥n `pgvector` para b√∫squeda vectorial
- **Neo4j** como base de datos del knowledge graph
- **Graphiti** (`graphiti-core`) para extracci√≥n autom√°tica de entidades y relaciones
- **OpenAI**, **Gemini** u **Ollama** como proveedor de LLM y embeddings (configurable)
- **Streamlit** + **Pyvis** para dashboard interactivo y visualizaci√≥n de grafos

---

## Estrategia de despliegue por fases

### Fase 1 ‚Äî Vector Only (Lanzamiento productivo)

**Objetivo:** velocidad m√°xima, costo m√≠nimo.

Solo se usa Postgres/pgvector. No se instala Neo4j, no se llama a Graphiti.

```bash
python -m poc.run_poc --ingest documents_to_index/ --skip-graphiti
```

---

### Fase 1.5 ‚Äî Metadata Enrichment (sin costo extra)

**Objetivo:** preparar la base de datos para que la migraci√≥n al grafo sea m√°s barata y precisa.

Durante la ingesta normal, el pipeline extrae autom√°ticamente metadatos estructurados **sin gastar ni un token de LLM**:

- **Personas detectadas:** regex sobre el bloque "Participantes:" o nombres Nombre Apellido en el texto.
- **Empresas y herramientas:** palabras con may√∫scula que aparecen ‚â•2 veces.
- **Segmentos temporales:** timestamps `[MM:SS]` del formato Novotalks, usados como l√≠mites de chunk.
- **Citas destacadas:** bloques `> "..."` del markdown.
- **`graphiti_ready_context`:** string pre-formateado con todo lo anterior, listo para Graphiti.

Ejemplo de contexto generado autom√°ticamente:
```
Document: Agust√≠n Linenberg - Ventas y Startups | Category: Podcast |
People: Agust√≠n Linenberg, Wences Casares |
Organizations: Aerolab, Clay, Lemon Wallet |
Topics: Perfil Personal; Emprender por Accidente; Ventas por Relaci√≥n
```

Frontmatter opcional para enriquecer metadatos:
```yaml
---
title: "Agust√≠n Linenberg: El Arte de Emprender"
category: Podcast
episode: "Novotalks #21"
guest: Agust√≠n Linenberg
host: Dami, Tommy
date: 2024-03-15
---
```

---

### Fase 2 ‚Äî Graph Hydration (Razonamiento profundo)

**Objetivo:** activar el knowledge graph para preguntas relacionales complejas.

```bash
# Preview: ver qu√© documentos se procesar√≠an
python -m poc.hydrate_graph --dry-run

# Ejecutar la migraci√≥n (secuencial, 5s entre episodios)
python -m poc.hydrate_graph

# Solo los primeros 10 (validar costos antes de escalar)
python -m poc.hydrate_graph --limit 10

# Sin pausa entre episodios (si el tier de API lo permite)
python -m poc.hydrate_graph --delay 0

# Re-procesar todos
python -m poc.hydrate_graph --reset-flags
```

---

## Instalaci√≥n y configuraci√≥n

### Opci√≥n A ‚Äî Docker (recomendado)

Todo el stack se levanta con un solo comando. Solo necesit√°s Docker y (opcionalmente) Ollama en el host.

```bash
git clone <repo>
cd poc-graphiti-agent

# 1. Crear .env desde el template
cp .env.example .env
# Editar .env con tus keys/passwords (ver opciones abajo)

# 2. Levantar todo (Postgres + Neo4j + Dashboard)
docker compose up --build -d

# 3. Abrir el dashboard
# http://localhost:8501
```

**Correr ingesta u otros comandos CLI dentro del container:**
```bash
docker compose exec app python -m poc.run_poc --ingest "documents_to_index" --all
docker compose exec app python -m poc.hydrate_graph --limit 5
```

**Conectar a Ollama del host:** El container usa `host.docker.internal` para alcanzar Ollama. Asegurate de que Ollama est√© corriendo en el host (`ollama serve`) y que `.env` tenga:
```
OPENAI_BASE_URL=http://host.docker.internal:11434/v1
```

**Parar todo:**
```bash
docker compose down          # Mantiene datos
docker compose down -v       # Borra datos (Postgres + Neo4j)
```

---

### Opci√≥n B ‚Äî Manual (sin Docker para la app)

#### Requisitos
- Python 3.10+
- Docker (para Postgres y Neo4j)
- (Opcional) [Ollama](https://ollama.com/) para modelos locales

#### 1. Clonar e instalar dependencias

```bash
git clone <repo>
cd poc-graphiti-agent
pip install -r requirements.txt
```

#### 2. Levantar Postgres y Neo4j con Docker

```bash
docker compose up postgres neo4j -d
```

#### 3. Configurar variables de entorno

```bash
cp .env.example .env
```

Editar `.env` seg√∫n el proveedor elegido.

> **Nota:** En setup manual, cambiar los hosts a `localhost`:
> - `NEO4J_URI=neo4j://127.0.0.1:7687`
> - `POSTGRES_HOST=localhost` / `POSTGRES_PORT=5435`
> - `OPENAI_BASE_URL=http://localhost:11434/v1` (si us√°s Ollama)

**OpenAI (cloud):**
```
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
DEFAULT_MODEL=gpt-5-mini
EMBEDDING_MODEL=text-embedding-3-small
```

**Ollama (local, $0 costo):**
```
LLM_PROVIDER=ollama
DEFAULT_MODEL=qwen2.5:3b
EMBEDDING_MODEL=nomic-embed-text
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama
NEO4J_URI=neo4j://127.0.0.1:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=adminadmin
POSTGRES_DSN=postgresql://user:password@localhost:5432/graphiti_poc
```

> **Nota Ollama:** Los modelos `qwen2.5:3b` y `nomic-embed-text` se descargan autom√°ticamente si no est√°n instalados. El sistema configura autom√°ticamente `small_model` para evitar que Graphiti intente usar `gpt-4.1-nano` y se pasa un timeout extendido (1800s) al cliente OpenAI para tolerar los tiempos de respuesta de modelos locales.

---

## C√≥mo correr el proyecto

### Flujo completo (ingesta + b√∫squedas + generaci√≥n)

```bash
python -m poc.run_poc --clear-logs --clear-db --ingest "documents_to_index" --all
```

> `--clear-db` ahora limpia **tanto Postgres como Neo4j** (nodos, relaciones e √≠ndices).

### Solo ingesta vectorial (sin Graphiti, m√°s r√°pido y barato)

```bash
python -m poc.run_poc --ingest documents_to_index/ --skip-graphiti
```

### Ingesta limitada (para pruebas r√°pidas)

```bash
python -m poc.run_poc --ingest "documents_to_index" --max-files 2 --all
```

### Solo b√∫squedas de prueba

```bash
python -m poc.run_poc --search
```

### Solo generaci√≥n de contenido

```bash
python -m poc.run_poc --generate
```

### Hidratar Graphiti desde Postgres (Fase 2)

```bash
python -m poc.hydrate_graph --limit 5
```

### Dashboard interactivo

```bash
python -m streamlit run dashboard/app.py
```

El dashboard incluye 7 tabs: Ingestion, Knowledge Base, Search, Generation, Analytics, Proyecciones y **Neo4j Graph** (ver secci√≥n 12).

---

## Estructura de archivos explicada

```
poc-graphiti-agent/
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Variables de entorno (Settings con Pydantic)
‚îÇ   ‚îú‚îÄ‚îÄ custom_openai_client.py   # Cliente OpenAI con fixes para gpt-5-mini y retry
‚îÇ   ‚îú‚îÄ‚îÄ db_utils.py               # Pool de conexiones Postgres + queries
‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py          # Cliente Gemini para Graphiti
‚îÇ   ‚îú‚îÄ‚îÄ graph_utils.py            # Wrapper Graphiti/Neo4j + monkey-patch UUID safety
‚îÇ   ‚îú‚îÄ‚îÄ models.py                 # Modelos Pydantic (SearchResult, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ tools.py                  # Herramientas de b√∫squeda (vector/graph/hybrid)
‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Dashboard Streamlit (7 tabs incluido Neo4j Graph)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Utilidades del dashboard
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py                # RecursiveChunker (chunk_size=800, overlap=100)
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py               # EmbeddingGenerator con cache (soporta Ollama)
‚îÇ   ‚îî‚îÄ‚îÄ ingest.py                 # Pipeline completo de ingesta
‚îú‚îÄ‚îÄ poc/
‚îÇ   ‚îú‚îÄ‚îÄ check_system.py           # Health check pre-vuelo
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Precios de modelos + defaults Ollama
‚îÇ   ‚îú‚îÄ‚îÄ content_generator.py      # Generador de contenido con l√≠mites de tokens
‚îÇ   ‚îú‚îÄ‚îÄ cost_calculator.py        # Calcula costo USD por operaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ hydrate_graph.py          # Migraci√≥n secuencial Postgres -> Neo4j
‚îÇ   ‚îú‚îÄ‚îÄ logging_utils.py          # Loggers CSV por tipo de operaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                  # Templates por formato (email, reel, historia)
‚îÇ   ‚îú‚îÄ‚îÄ queries.py                # 20 queries de prueba (vector/graph/hybrid)
‚îÇ   ‚îú‚îÄ‚îÄ run_poc.py                # Entrypoint principal (--max-files, --clear-db)
‚îÇ   ‚îî‚îÄ‚îÄ token_tracker.py          # Singleton de tracking de tokens y costos
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ neo4j_diagnostic.py       # Script de diagn√≥stico Neo4j (conteo, labels, edges)
‚îÇ   ‚îî‚îÄ‚îÄ neo4j_viewer.py           # Visualizador standalone (Streamlit + Pyvis)
‚îú‚îÄ‚îÄ documents_to_index/           # Documentos .md a ingestar
‚îú‚îÄ‚îÄ logs/                         # CSVs de m√©tricas generados autom√°ticamente
‚îú‚îÄ‚îÄ Dockerfile                    # Build multi-stage (Python 3.13-slim)
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ docker-compose.yml            # Postgres + Neo4j + App (3 servicios)
‚îú‚îÄ‚îÄ .env.example                  # Template de variables de entorno para Docker
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## Flujo de datos de punta a punta

### Ingesta (`ingest.py`)

1. **Lectura:** carga el archivo `.md` en memoria.
2. **Deduplicaci√≥n:** calcula SHA256 del contenido; si ya existe en Postgres, saltea.
3. **Frontmatter:** extrae metadatos YAML del bloque `---`.
4. **Extracci√≥n heur√≠stica (sin LLM):** personas, empresas, citas, segmentos temporales.
5. **Strip de Markdown:** elimina `##`, `**`, `>`, `-`, etc. antes de enviar a Graphiti (ahorra 5-15% de tokens).
6. **Chunking:** divide el texto en chunks de 800 chars con 100 de overlap.
7. **Embedding:** genera vectores en batch (una sola llamada a la API).
8. **Postgres:** guarda documento y chunks con sus embeddings.
9. **Graphiti (opcional):** env√≠a el texto (truncado a 6.000 chars) para extracci√≥n de entidades y relaciones.

### B√∫squeda (`tools.py`)

- **`vector_search_tool`:** embeddea la query (con cache), busca por cosine similarity en Postgres, retorna top-3.
- **`graph_search_tool`:** busca hechos y relaciones en Neo4j via Graphiti, retorna top-3.
- **`hybrid_search_tool`:** combina vector + full-text con Reciprocal Rank Fusion (RRF), retorna top-3.

### Generaci√≥n (`content_generator.py`)

Recibe los resultados de b√∫squeda como contexto, los inyecta en el template del formato solicitado, y llama al LLM con un l√≠mite de tokens por formato (email: 300, reel: 250, historia: 500).

---

## Sistema de m√©tricas y costos

Cada operaci√≥n genera una fila en los logs CSV:

| Log | Contenido |
|-----|-----------|
| `logs/ingesta_log.csv` | Costo por documento ingestado, tokens usados, tiempo |
| `logs/busqueda_log.csv` | Costo por query, tipo de b√∫squeda, latencia |
| `logs/generacion_log.csv` | Costo por pieza generada, tokens in/out, formato |

---

## Optimizaciones de costo implementadas

Esta secci√≥n documenta todos los cambios t√©cnicos realizados para reducir el consumo de tokens y corregir errores que causaban p√©rdidas de costo.

### Bugs corregidos

#### BUG 1: UnicodeEncodeError en Windows ‚Äî `custom_openai_client.py`
**S√≠ntoma:** El sistema loggeaba un "Logging error" al arrancar en Windows y el mensaje de inicializaci√≥n de Graphiti fallaba silenciosamente.

**Causa ra√≠z:** El mensaje de log usaba la flecha unicode `‚Üí` (U+2192). La consola de Windows con encoding `cp1252` no puede encodear ese car√°cter, y el m√≥dulo `logging` lanzaba una excepci√≥n interna.

**Fix:** Todos los mensajes de log ahora usan √∫nicamente caracteres ASCII (`->`).

---

#### BUG 2: LengthFinishReasonError en todos los episodios ‚Äî `custom_openai_client.py`
**S√≠ntoma:** 100% de las llamadas a `add_episode()` fallaban con:
```
LengthFinishReasonError: Could not parse response content as the length limit was reached
completion_tokens=2048, reasoning_tokens=2048
```

**Causa ra√≠z:** `gpt-5-mini` es un **modelo de razonamiento** (familia `o1`). Antes de producir output visible, consume *reasoning_tokens* de forma interna. Con el l√≠mite heredado de `graphiti-core` (`DEFAULT_MAX_TOKENS = 2048`), el modelo usaba los 2048 tokens **enteros** en razonamiento, dejando 0 tokens para el JSON estructurado que Graphiti necesita parsear.

El log lo confirmaba: `reasoning_tokens=2048` en cada intento fallido.

**Fix:** Para modelos de razonamiento (`gpt-5-*`, `o1-*`), se fuerza `max_completion_tokens = 8192`. El peor caso observado en los logs (prompt ~19.6k tokens) requiere ~4.000-5.000 reasoning tokens + ~400 para el JSON de output. Los tokens no usados no se facturan.

---

#### BUG 3: CancelledError en archivos pendientes ‚Äî `ingestion/ingest.py`
**S√≠ntoma:** Dos archivos fallaban por BUG 2 y los tres archivos restantes recib√≠an `CancelledError` en lugar de procesarse normalmente.

**Causa ra√≠z:** `ingest_file()` hac√≠a `raise` en su bloque `except`, propagando la excepci√≥n hacia el `asyncio.gather()`. Aunque `gather()` usaba `return_exceptions=True`, en Python 3.13 las tareas que estaban *esperando adquirir el sem√°foro* recib√≠an `CancelledError` al detectar que el sem√°foro fue liberado por una excepci√≥n.

**Fix:** `ingest_file()` ya no hace `re-raise`. Loggea el error con `logger.exception()` y retorna `None`. El `gather()` ve `None` (no `Exception`) y contin√∫a con los archivos restantes sin interrupciones.

---

#### BUG 4: NameError en retries por import faltante ‚Äî `custom_openai_client.py`
**S√≠ntoma:** Los retries con backoff exponencial ante errores 429 nunca se ejecutaban; el sistema lanzaba `NameError: name 'asyncio' is not defined` en `_make_request_with_retry()`.

**Causa ra√≠z:** `import asyncio` solo estaba dentro del m√©todo `setup()` (scope local). Cuando `_make_request_with_retry()` llamaba a `asyncio.sleep(delay)`, el nombre `asyncio` no exist√≠a en el scope del m√≥dulo.

**Fix:** Se movi√≥ `import asyncio` al nivel de m√≥dulo (l√≠nea 1). Adem√°s, se agreg√≥ inicializaci√≥n perezosa del sem√°foro de concurrencia (`_semaphore`) dentro de `_make_request_with_retry()` para que el cliente funcione correctamente incluso si `setup()` nunca se llama.

---

#### BUG 5: Graphiti solo muestra un episodio ‚Äî `graph_utils.py` + `hydrate_graph.py`
**S√≠ntoma:** Al consultar episodios despu√©s de la hidrataci√≥n, solo aparec√≠a un documento (ej. "Alex") en lugar de todos los documentos indexados.

**Causa ra√≠z:** El archivo `graph_utils.py` fue reescrito con una clase `GraphManager` (basada en instancias), pero el resto del c√≥digo (`tools.py`, `ingest.py`, `run_poc.py`, `check_system.py`) importa `GraphClient` (singleton con `@classmethod`). Esto significa que:
1. Los imports fallaban silenciosamente o el sistema usaba una instancia aislada.
2. `add_episode()` no pasaba `group_id`, y cada episodio terminaba en un grupo distinto.
3. La consulta de episodios no usaba `group_ids=None` para recuperar todos los grupos.

**Fix:** Se restaur√≥ la clase `GraphClient` singleton compatible con el resto del c√≥digo, con estas mejoras:
- `add_episode()` ahora acepta y pasa `group_id` (default: `"hybrid_rag_documents"`) para que todos los documentos pertenezcan al mismo grupo.
- Se agreg√≥ `get_all_episodes(group_ids=None)` que usa `client.get_episodes()` para recuperar episodios de **todos** los grupos.
- Se agregaron m√©todos `reset()` y `_build_client()` requeridos por `run_poc.py` y `check_system.py`.
- `hydrate_graph.py` fue actualizado para usar `GraphClient` en lugar de `GraphManager`.

---

#### BUG 7: KeyError en resolve_extracted_edges con LLMs peque√±os ‚Äî `graph_utils.py`
**S√≠ntoma:** La ingesta de ciertos documentos (ej. `lucas.md`) fallaba con:
```
KeyError: '78edfb08-3cab-4fb4-a9fb-5a88af334189'
```
en `graphiti_core/utils/maintenance/edge_operations.py` l√≠nea 317.

**Causa ra√≠z:** Modelos peque√±os como `qwen2.5:3b` a veces generan edges (relaciones) que referencian UUIDs de entidades que no existen en la lista de entidades extra√≠das. El c√≥digo upstream de Graphiti hace un `dict[uuid]` directo sin verificar si el UUID existe, causando un `KeyError` que aborta la ingesta completa del documento.

**Fix:** Se implement√≥ un **monkey-patch** en `agent/graph_utils.py` que intercepta `resolve_extracted_edges` antes de que procese los edges:
1. Construye un set de UUIDs v√°lidos desde la lista de entidades.
2. Filtra los edges, descartando aquellos con `source_node_uuid` o `target_node_uuid` inexistentes.
3. Loggea un `WARNING` por cada edge descartado.
4. Pasa solo los edges v√°lidos a la funci√≥n original.

El patch se aplica tanto al m√≥dulo `edge_operations` como al import directo en `graphiti_core.graphiti` (que usa `from ... import resolve_extracted_edges`). El documento se ingesta correctamente aunque pierde algunos edges que el LLM gener√≥ incorrectamente.

---

#### BUG 6: Retries infinitos ante quota agotada ‚Äî `custom_openai_client.py`, `embedder.py`, `run_poc.py`
**S√≠ntoma:** Cuando la cuenta de OpenAI no tiene cr√©ditos, la API responde con 429 y `code: insufficient_quota`. El sistema reintentaba indefinidamente (hasta 5 veces con delays crecientes) sin jamas poder triunfar, y terminaba con un `KeyboardInterrupt` del usuario.

**Causa ra√≠z:** OpenAI usa el mismo c√≥digo HTTP 429 para dos tipos de error muy distintos: (1) rate limit transit√≥rio (se recupera solo) y (2) quota agotada (requiere acci√≥n del usuario). El c√≥digo anterior no diferenciaba entre ellos.

**Fix:**
- `custom_openai_client.py`: En el handler de `RateLimitError`, se verifica `e.code == 'insufficient_quota'` antes de calcular el backoff. Si es quota, se loggea un mensaje `CRITICAL` con el link de billing y se re-lanza inmediatamente sin reintentos.
- `embedder.py`: Mismo chequeo en `generate_embeddings_batch()` y `_embed_one()` para errores de embedding.
- `run_poc.py`: Se separa `_main()` de `main()`. El wrapper `main()` captura cualquier excepci√≥n, detecta si es quota (por `e.code` o por contenido del mensaje), muestra un banner `FATAL ERROR` con instrucci√≥n clara y sale con `SystemExit(1)` en lugar de crashear con `CancelledError` o `KeyboardInterrupt`.

---

### Optimizaciones de costo

| M√≥dulo | Cambio | Ahorro estimado |
|--------|--------|-----------------|
| `agent/custom_openai_client.py` | `small_model` forzado a `medium_model` (evita `gpt-4.1-nano` con l√≠mite TPM 200k) | Elimina rate limits en ingesta |
| `agent/custom_openai_client.py` | Retry con backoff exponencial ante 429 (5 intentos: 10/20/40/80/160s) | Recupera episodios que antes se perd√≠an |
| `agent/graph_utils.py` | Truncado de `episode_body` a **6.000 chars** antes de `add_episode()` | ~60% de tokens en Graphiti |
| `agent/tools.py` | Resultados de b√∫squeda: **5 ‚Üí 3** (vector, hybrid, graph) | ~400 tokens de input por query |
| `ingestion/ingest.py` | Strip de sintaxis Markdown antes de Graphiti | 5-15% de tokens por episodio |
| `ingestion/chunker.py` | `chunk_size`: 1000 ‚Üí **800**, `chunk_overlap`: 200 ‚Üí **100** | ~50% de tokens duplicados en embedding |
| `ingestion/embedder.py` | Cache LRU de **256 entradas** para queries repetidas | Queries repetidas: $0 y latencia cero |
| `poc/content_generator.py` | `max_tokens` por formato (email: 300, reel: 250, historia: 500) | 50-80% del costo de generaci√≥n |
| `poc/hydrate_graph.py` | Procesamiento **secuencial** con delay configurable (`--delay 5`) | Elimina rate limits en hidrataci√≥n |

---

## Criterios de √©xito

### GO ‚Äî Seguir adelante con producci√≥n

- Costo de ingesta < $0.10 por documento
- Costo promedio por query < $0.001
- Latencia de b√∫squeda < 2 segundos
- Proyecci√≥n mensual (250 docs) < $100

### OPTIMIZE ‚Äî Ajustar antes de escalar

- Costo por documento: $0.10 - $0.25
- Proyecci√≥n mensual: $100 - $200

### STOP ‚Äî Re-evaluar arquitectura

- Costo por documento > $0.25
- Proyecci√≥n mensual > $200

---

## Preguntas frecuentes

**¬øPor qu√© el chunk_size es 800 y no 1000?**
Chunks m√°s peque√±os producen recuperaci√≥n m√°s precisa (retorna solo la secci√≥n relevante, no p√°rrafos enteros). El ahorro en tokens de contexto en generaci√≥n supera el leve aumento en costos de embedding de ingesta (que ocurre una sola vez).

**¬øPor qu√© el overlap es 100 y no 200?**
El overlap existe para evitar que ideas queden cortadas sin contexto. Pero cada car√°cter de overlap se embeddea dos veces (en el chunk anterior y en el siguiente). Con overlap=100 sobre chunk_size=800, solo el 12.5% de los tokens se duplican (antes: 20%). La calidad de recuperaci√≥n no cambia materialmente para textos conversacionales.

**¬øPor qu√© se trunca el texto antes de enviarlo a Graphiti?**
Graphiti realiza ~30 llamadas LLM internas por episodio, y cada una recibe el texto completo como contexto. Las entidades clave de un documento t√≠pico siempre est√°n en las primeras 6.000 caracteres. Truncar a ese l√≠mite reduce ~60% del costo de Graphiti sin impactar la calidad del grafo.

**¬øPor qu√© `gpt-5-mini` necesita `max_completion_tokens = 8192`?**
`gpt-5-mini` pertenece a la familia de modelos de razonamiento (`o1`). Antes de producir output visible, consume *reasoning tokens* de forma interna. Con el l√≠mite por defecto de 2048 tokens, el modelo usa todos los tokens en razonamiento y no le queda espacio para generar el JSON estructurado que Graphiti necesita. Aumentar el l√≠mite a 8192 da el espacio necesario; los tokens no usados no se cobran.

**Por que la hidratacion es secuencial y no paralela?**
`add_episode()` dispara internamente ~30 llamadas LLM en paralelo. Si se procesan 2-3 episodios simult√°neos, se multiplican las llamadas paralelas por 2-3, agotando el l√≠mite de tokens por minuto (TPM) en segundos. El procesamiento secuencial con un delay de 5 segundos entre episodios permite que la ventana de TPM se renueve parcialmente y elimina los errores 429.

---

## 12. Nuevos Componentes ‚Äî Motor IA Novolabs

Esta secci√≥n documenta las 5 nuevas capas funcionales agregadas al proyecto.

### Tarea 1 ‚Äî Capa de Servicios (`services/`)

Una capa intermedia entre el dashboard/API y la l√≥gica interna. Separa "qu√© hace el sistema" de "c√≥mo lo hace internamente", facilitando crear una API REST en el futuro sin tocar el dashboard.

| Archivo | Qu√© hace |
|---|---|
| `services/ingestion_service.py` | Orquesta la ingesta: deduplicaci√≥n, chunking, embeddings, almacenamiento |
| `services/generation_service.py` | Delega al agente correcto y retorna el output estructurado |
| `services/search_service.py` | Fachada para los 4 modos de b√∫squeda (vector, grafo, h√≠brido, h√≠brido-real) |

### Tarea 2 ‚Äî Fuentes de Documentos Enchufables (`ingestion/sources/`)

El sistema ahora puede ingestar desde cualquier origen de datos sin modificar el pipeline principal.

| Archivo | Qu√© hace |
|---|---|
| `ingestion/sources/base.py` | Clase abstracta `DocumentSource` ‚Äî define el contrato que toda fuente debe cumplir |
| `ingestion/sources/local_file_source.py` | Lee archivos `.md` desde una carpeta local (implementado) |
| `ingestion/sources/google_drive_source.py` | Stub para futura integraci√≥n con Google Drive (Fase 1) |

**Uso:** `from ingestion.ingest import ingest_from_source` ‚Äî acepta cualquier `DocumentSource`.

### Tarea 3 ‚Äî Agentes de Generaci√≥n Estructurada (`poc/agents/`)

Cada formato de contenido tiene su propio agente con instrucciones espec√≠ficas (SOP) y validaci√≥n de calidad. El output no es texto libre ‚Äî es un objeto JSON con campos definidos (Hook, Script, CTA, etc.).

| Agente | Formato que genera |
|---|---|
| `ReelCTAAgent` | Guion de reel (Instagram/TikTok) con CTA |
| `HistoriaAgent` | Secuencia de 5-7 Stories de Instagram |
| `EmailAgent` | Email de newsletter o outreach |
| `ReelLeadMagnetAgent` | Reel que promociona un recurso gratuito |
| `AdsAgent` | Copy para Meta Ads o Google Ads |

Los archivos de instrucciones (SOPs) est√°n en `config/sops/` y pueden editarse sin tocar c√≥digo.

**Uso en dashboard:** Tab "Generation" ‚Üí secci√≥n "Agentes Estructurados" ‚Üí elegir formato ‚Üí completar campos ‚Üí bot√≥n "Generar con Agente Estructurado".

**Uso en CLI:** `python -m poc.run_poc --generate-structured --formato reel_cta --topic "tu tema"`

### Tarea 4 ‚Äî Control de Presupuesto (`poc/budget_guard.py`)

Evita sorpresas de facturaci√≥n al monitorear el gasto mensual acumulado y cambiar autom√°ticamente al modelo m√°s barato cuando se supera el 90% del l√≠mite.

| Variable de entorno (`.env`) | Valor por defecto | Qu√© hace |
|---|---|---|
| `MONTHLY_BUDGET_USD` | `10.0` | L√≠mite mensual en USD. `0` = desactivado |
| `FALLBACK_MODEL` | `gpt-4o-mini` | Modelo barato que se activa al llegar al 90% |
| `BUDGET_TRACKING_FILE` | `logs/monthly_budget.json` | Donde se guarda el gasto acumulado |

**Alertas autom√°ticas:** WARNING al 70%, CRITICAL al 90% (con cambio de modelo).

El presupuesto se muestra en el dashboard (tab Analytics ‚Üí secci√≥n "Estado del Presupuesto").

### Tarea 5 ‚Äî Motor de B√∫squeda H√≠brido Real (`agent/retrieval_engine.py`)

La b√∫squeda h√≠brida existente combina vector y FTS dentro de Postgres solamente. `RetrievalEngine` agrega un tercer paso: usa Neo4j para identificar qu√© documentos son conceptualmente relevantes, y luego va a Postgres a buscar el texto literal de esos documentos.

```
Query del usuario
      ‚îÇ
      ‚ñº
 Neo4j / Graphiti   ‚Üí identifica "qu√© documentos mencionan este concepto"
      ‚îÇ
      ‚ñº
  PostgreSQL       ‚Üí trae los chunks literales de esos documentos
      ‚îÇ
      ‚ñº
 Resultado enriquecido (contexto conceptual + texto literal)
```

**Cu√°ndo usar `hybrid_real`:** Cuando la query es relacional ("qu√© dijo X sobre Y", "qu√© documentos hablan de Z"). Para queries sem√°nticas directas, `hybrid` sigue siendo m√°s r√°pido.

**Fallback autom√°tico:** Si Neo4j no retorna resultados, el motor cae autom√°ticamente a b√∫squeda vectorial.

---

## 13. Soporte Ollama (Modelos Locales)

El sistema puede correr 100% local usando [Ollama](https://ollama.com/), eliminando costos de API. Para activarlo:

1. Instalar Ollama y descargar los modelos:
   ```bash
   ollama pull qwen2.5:3b
   ollama pull nomic-embed-text
   ```

2. Configurar `.env` con `LLM_PROVIDER=ollama` (ver secci√≥n Instalaci√≥n).

### Archivos modificados para Ollama

| Archivo | Cambio |
|---|---|
| `poc/config.py` | Validador `_resolve_gemini_defaults` extendido para detectar `ollama` y setear defaults (`qwen2.5:3b`, `nomic-embed-text`, `http://localhost:11434/v1`). Precios de modelos Ollama agregados como `$0.0`. |
| `agent/graph_utils.py` | Branch `elif provider == "ollama"` en `get_client()` que configura `OpenAIClient` y `OpenAIEmbedder` con la URL de Ollama. Se pasa `small_model=settings.DEFAULT_MODEL` a `LLMConfig` para evitar que Graphiti use `gpt-4.1-nano`. Timeout del cliente extendido a 1800s. |
| `ingestion/embedder.py` | Soporte Ollama en `Embedder.__init__()` configurando `AsyncOpenAI` con `base_url` de Ollama. |
| `agent/db_utils.py` | Detecci√≥n de dimensi√≥n de embedding: 768 para Ollama/Gemini (nomic-embed-text), 1536 para OpenAI. Auto-recreaci√≥n del schema si las dimensiones no coinciden. |
| `agent/custom_openai_client.py` | `base_url` configurable desde `OPENAI_BASE_URL` env var para redirigir a Ollama. |

### Limitaciones conocidas con Ollama

- **Velocidad:** Modelos locales son ~10-50x m√°s lentos que APIs cloud. Una ingesta de 5 documentos puede tomar ~15-20 minutos.
- **Calidad de edges:** `qwen2.5:3b` genera UUIDs inconsistentes en ~10-20% de los documentos. El monkey-patch (BUG 7) mitiga esto saltando edges rotos.
- **Max tokens:** El modelo puede exceder el l√≠mite de `max_tokens=16384`, causando retries. Esto es normal y el sistema reintenta autom√°ticamente.

---

## 14. Neo4j Graph Explorer (Dashboard Tab)

El tab **üîµ Neo4j Graph** en el dashboard provee una interfaz completa para explorar el knowledge graph:

### M√©tricas
- Conteo de nodos, relaciones, episodios y entidades.

### Sub-tabs

| Sub-tab | Qu√© muestra |
|---|---|
| **Interactive Graph** | Grafo interactivo con [Pyvis](https://pyvis.readthedocs.io/). Nodos coloreados por label (Entity=azul, Episodic=naranja, Community=verde). Filtro por label, slider de max nodos, toggle de physics. |
| **Episodes** | Lista de documentos ingresados con metadata (nombre, fecha de creaci√≥n, group_id, source_description). |
| **Details** | Breakdown de labels de nodos y tipos de relaci√≥n con conteos. |
| **Cypher Query** | Permite ejecutar cualquier query Cypher directo contra Neo4j. Resultados en tabla interactiva. |

### Herramientas de diagn√≥stico (CLI)

```bash
# Diagn√≥stico completo (nodos, labels, episodios, entidades, edges)
python tools/neo4j_diagnostic.py

# Quick check (conteos b√°sicos)
python tools/_quick_check.py
```
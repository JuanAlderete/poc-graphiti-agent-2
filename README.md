# POC: Graphiti Agent ‚Äî RAG H√≠brido con Despliegue por Fases

Esta prueba de concepto valida si la arquitectura **Graphiti + PostgreSQL/pgvector** es econ√≥micamente viable para producci√≥n. El sistema combina la velocidad de b√∫squeda vectorial de Postgres con la capacidad de razonamiento relacional de Graphiti/Neo4j, y est√° dise√±ado para activarse en etapas: arranc√°s barato con solo Postgres y activ√°s el grafo cuando el negocio lo justifica.

---

## √çndice

1. [¬øQu√© problema resuelve?](#qu√©-problema-resuelve)
2. [Arquitectura general](#arquitectura-general)
3. [Estrategia de despliegue por fases](#estrategia-de-despliegue-por-fases)
4. [Instalaci√≥n y configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
5. [C√≥mo correr el proyecto](#c√≥mo-correr-el-proyecto)
6. [Estructura de archivos explicada](#estructura-de-archivos-explicada)
7. [Flujo de datos de punta a punta](#flujo-de-datos-de-punta-a-punta)
8. [Sistema de m√©tricas y costos](#sistema-de-m√©tricas-y-costos)
9. [Dashboard](#dashboard)
10. [Criterios de √©xito (GO / OPTIMIZE / STOP)](#criterios-de-√©xito)
11. [Preguntas frecuentes](#preguntas-frecuentes)

---

## ¬øQu√© problema resuelve?

El cliente tiene una base de conocimiento (transcripciones de podcasts, gu√≠as, playbooks) y necesita un agente que pueda responder preguntas usando esa informaci√≥n. La duda es: **¬øcu√°nto cuesta realmente operar esto a escala?**

Este POC responde esa pregunta midiendo el costo exacto (en USD) de cada operaci√≥n: ingestar un documento, hacer una b√∫squeda, generar un email. Con esos datos, el dashboard proyecta el gasto mensual y anual bajo distintos escenarios.

La arquitectura tambi√©n resuelve un problema t√©cnico: ¬øc√≥mo activar un knowledge graph sin tirar todo lo que ya est√° corriendo? La respuesta es la **migraci√≥n por hidrataci√≥n** ‚Äî los documentos ya guardados en Postgres se pueden "hidratar" a Neo4j en un paso separado, sin re-ingestar archivos ni interrumpir el servicio.

---

## Arquitectura general

```
Documentos (.md)
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ingestion/ingest.py       ‚îÇ  Pipeline de ingesta
‚îÇ                             ‚îÇ
‚îÇ  1. Parsea frontmatter YAML ‚îÇ
‚îÇ  2. Extrae entidades (gratis‚îÇ  ‚Üê sin LLM, solo regex
‚îÇ  3. Chunking por segmentos  ‚îÇ
‚îÇ  4. Genera embeddings       ‚îÇ  ‚Üê OpenAI / Gemini
‚îÇ  5. Guarda en Postgres      ‚îÇ
‚îÇ  6. (Opcional) ‚Üí Graphiti   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                ‚îÇ
         ‚ñº                ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ PostgreSQL ‚îÇ    ‚îÇ  Neo4j    ‚îÇ
  ‚îÇ (pgvector) ‚îÇ    ‚îÇ(Graphiti) ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  agent/tools.py ‚îÇ  Capa de b√∫squeda
        ‚îÇ                 ‚îÇ
        ‚îÇ ‚Ä¢ vector_search ‚îÇ  ‚Üê cosine similarity
        ‚îÇ ‚Ä¢ graph_search  ‚îÇ  ‚Üê relaciones/entidades
        ‚îÇ ‚Ä¢ hybrid_search ‚îÇ  ‚Üê RRF (combina ambas)
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ poc/content_    ‚îÇ  Generaci√≥n de contenido
        ‚îÇ generator.py    ‚îÇ  (emails, reels, historias)
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ poc/token_      ‚îÇ  Tracking de costos
        ‚îÇ tracker.py      ‚îÇ  (cada operaci√≥n loguea USD)
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Stack tecnol√≥gico:**
- **Python 3.10+** con `asyncio` / `asyncpg` para I/O no bloqueante
- **PostgreSQL** con extensi√≥n `pgvector` para b√∫squeda vectorial
- **Neo4j** como base de datos del knowledge graph
- **Graphiti** (`graphiti-core`) para extracci√≥n autom√°tica de entidades y relaciones
- **OpenAI** o **Gemini** como proveedor de LLM y embeddings (configurable)
- **Streamlit** para el dashboard de m√©tricas

---

## Estrategia de despliegue por fases

El proyecto est√° dise√±ado para crecer sin deuda t√©cnica. Cada fase es un subset de la siguiente.

### Fase 1 ‚Äî Vector Only (Lanzamiento productivo)

**Objetivo:** velocidad m√°xima, costo m√≠nimo.

Solo se usa Postgres/pgvector. No se instala Neo4j, no se llama a Graphiti. El costo de ingesta es casi exclusivamente el embedding del texto (~$0.02/1M tokens).

```bash
# Ingestar documentos sin Graphiti
python -m poc.run_poc --ingest documents_to_index/ --skip-graphiti

# B√∫squedas disponibles: vector y hybrid (RRF vector + full-text)
```

Cu√°ndo usar esta fase: arranque del proyecto, validaci√≥n de calidad de b√∫squeda, cuando el volumen de datos es bajo y las preguntas son "sem√°nticas" (no relacionales).

---

### Fase 1.5 ‚Äî Metadata Enrichment (sin costo extra)

**Objetivo:** preparar la base de datos para que la migraci√≥n al grafo sea m√°s barata y precisa.

Durante la ingesta normal (Fase 1), el pipeline ahora extrae autom√°ticamente metadatos estructurados **sin gastar ni un token de LLM**:

- **Personas detectadas**: regex sobre el bloque "Participantes:" o nombres Nombre Apellido en el texto.
- **Empresas y herramientas**: palabras con may√∫scula que aparecen ‚â•2 veces (Aerolab, Clay, Neo4j, etc.).
- **Segmentos temporales**: timestamps `[MM:SS]` del formato Novotalks, usados como l√≠mites de chunk.
- **Citas destacadas**: bloques `> "..."` del markdown.
- **`graphiti_ready_context`**: un string pre-formateado con todo lo anterior, listo para inyectar en Graphiti.

Ejemplo de lo que se genera autom√°ticamente para un documento:

```
Document: Agust√≠n Linenberg - Ventas y Startups | Category: Podcast |
People: Agust√≠n Linenberg, Wences Casares, Dami, Tommy |
Organizations: Aerolab, Clay, Lemon Wallet, Neo4j |
Topics: Perfil Personal; Emprender por Accidente; Ventas por Relaci√≥n
```

Este contexto se guarda en `documents.metadata` (columna JSONB). Cuando llegue Fase 2, Graphiti lo recibe como `source_description` en `add_episode()` y puede enfocar la extracci√≥n directamente en las entidades correctas, en lugar de inferirlas desde cero.

C√≥mo agregar frontmatter a tus documentos para enriquecer a√∫n m√°s los metadatos:

```yaml
---
title: "Agust√≠n Linenberg: El Arte de Emprender"
category: Podcast
episode: "Novotalks #21"
guest: Agust√≠n Linenberg
host: Dami, Tommy
date: 2024-03-15
---
```

---

### Fase 2 ‚Äî Graph Hydration (Razonamiento profundo)

**Objetivo:** activar el knowledge graph para preguntas relacionales complejas.

En lugar de re-ingestar todos los archivos, se usa el script `poc/hydrate_graph.py` que lee los documentos ya guardados en Postgres y los env√≠a a Graphiti. El proceso es **reanudable**: si se corta, la pr√≥xima ejecuci√≥n contin√∫a desde donde qued√≥ usando el flag `metadata->>'graph_ingested'`.

```bash
# Preview: ver qu√© documentos se procesar√≠an y con qu√© contexto
python -m poc.hydrate_graph --dry-run

# Ejecutar la migraci√≥n
python -m poc.hydrate_graph

# Solo los primeros 10 documentos (para validar costos antes de escalar)
python -m poc.hydrate_graph --limit 10

# Re-procesar todos (ignorar el flag de ya-hidratado)
python -m poc.hydrate_graph --reset-flags
```

Una vez hidratado el grafo, est√°n disponibles los tres modos de b√∫squeda:

```bash
# B√∫squeda vectorial (r√°pida, sem√°ntica)
# B√∫squeda en grafo (relaciones, entidades, hechos)
# B√∫squeda h√≠brida (combina ambas con RRF)
python -m poc.run_poc --search
```

---

## Instalaci√≥n y configuraci√≥n

### Requisitos

- Python 3.10+
- Docker (para Postgres y Neo4j)

### 1. Clonar e instalar dependencias

```bash
git clone <repo>
cd poc-graphiti-agent
pip install -r requirements.txt
```

### 2. Levantar los servicios con Docker

```bash
docker-compose up -d
```

Esto levanta:
- **PostgreSQL** con extensi√≥n `pgvector` en el puerto 5432
- **Neo4j** en el puerto 7687 (bolt) y 7474 (browser web)

### 3. Configurar el archivo `.env`

Copiar `.env.example` y completar:

```env
# Proveedor LLM: "openai" o "gemini"
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...

# Alternativa Gemini
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=AI...

# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
POSTGRES_DB=graphiti_poc
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# Neo4j (solo necesario en Fase 2)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Modelos (opcional, se auto-configuran seg√∫n el proveedor)
# DEFAULT_MODEL=gpt-4o-mini
# EMBEDDING_MODEL=text-embedding-3-small
```

**Nota sobre modelos:** si `LLM_PROVIDER=gemini`, el sistema autom√°ticamente usa `gemini-1.5-flash` y `text-embedding-004`. Si `LLM_PROVIDER=openai`, usa `gpt-5-mini` y `text-embedding-3-small`. Se pueden sobreescribir en el `.env`.

### 4. Verificar que todo funciona

```bash
python -m poc.check_system
```

Esto verifica conexi√≥n a Postgres, Neo4j (si est√° configurado), y que las API keys sean v√°lidas.

---

## C√≥mo correr el proyecto

### Flujo completo (recomendado para el POC)

```bash
# Paso 1: Ingestar documentos (Fase 1 - sin grafo)
python -m poc.run_poc --ingest documents_to_index/ --skip-graphiti

# Paso 2: Correr b√∫squedas de prueba
python -m poc.run_poc --search --skip-graphiti

# Paso 3: Generar contenido de prueba
python -m poc.run_poc --generate

# Paso 4: Ver m√©tricas en el dashboard
streamlit run dashboard/app.py
```

### Comandos individuales √∫tiles

```bash
# Correr todo en un comando
python -m poc.run_poc --all --ingest documents_to_index/

# Limpiar la base de datos y los logs antes de un run fresco
python -m poc.run_poc --clear-db --clear-logs --ingest documents_to_index/ --skip-graphiti

# Saltear el health check (m√°s r√°pido si ya sab√©s que todo est√° up)
python -m poc.run_poc --ingest documents_to_index/ --skip-checks --skip-graphiti

# Hidrataci√≥n al grafo (Fase 2)
python -m poc.hydrate_graph --dry-run    # primero revisar
python -m poc.hydrate_graph              # ejecutar

# Solo b√∫squedas con grafo activado
python -m poc.run_poc --search
```

---

## Estructura de archivos explicada

```
poc-graphiti-agent/
‚îÇ
‚îú‚îÄ‚îÄ agent/                      ‚Üê Capa de acceso a datos y b√∫squeda
‚îÇ   ‚îú‚îÄ‚îÄ config.py               ‚Üê Re-exporta poc/config.py (backward compat)
‚îÇ   ‚îú‚îÄ‚îÄ db_utils.py             ‚Üê Pool de conexiones Postgres + helpers CRUD
‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py        ‚Üê Adaptador LLMClient de Graphiti para Gemini
‚îÇ   ‚îú‚îÄ‚îÄ graph_utils.py          ‚Üê Wrapper de Graphiti/Neo4j
‚îÇ   ‚îú‚îÄ‚îÄ models.py               ‚Üê Modelos Pydantic (SearchResult, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ tools.py                ‚Üê Las 3 herramientas de b√∫squeda del agente
‚îÇ
‚îú‚îÄ‚îÄ ingestion/                  ‚Üê Pipeline de procesamiento de documentos
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py              ‚Üê Divide texto en chunks con overlap
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py             ‚Üê Genera vectores (OpenAI o Gemini), singleton
‚îÇ   ‚îî‚îÄ‚îÄ ingest.py               ‚Üê Orquesta todo el pipeline de ingesta
‚îÇ
‚îú‚îÄ‚îÄ poc/                        ‚Üê Scripts del POC y sistema de m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ config.py               ‚Üê Configuraci√≥n central + precios de modelos
‚îÇ   ‚îú‚îÄ‚îÄ token_tracker.py        ‚Üê Singleton thread-safe para contar tokens
‚îÇ   ‚îú‚îÄ‚îÄ cost_calculator.py      ‚Üê Calcula USD a partir de tokens y modelo
‚îÇ   ‚îú‚îÄ‚îÄ logging_utils.py        ‚Üê Loggers CSV para ingesta, b√∫squeda y generaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ content_generator.py    ‚Üê Genera contenido usando el LLM configurado
‚îÇ   ‚îú‚îÄ‚îÄ hydrate_graph.py        ‚Üê Migraci√≥n Postgres ‚Üí Graphiti (Fase 2)
‚îÇ   ‚îú‚îÄ‚îÄ run_poc.py              ‚Üê Script principal, entry point del POC
‚îÇ   ‚îú‚îÄ‚îÄ check_system.py         ‚Üê Health check de conexiones y variables
‚îÇ   ‚îú‚îÄ‚îÄ queries.py              ‚Üê 20 queries de prueba (vector, graph, hybrid)
‚îÇ   ‚îî‚îÄ‚îÄ prompts/                ‚Üê Templates de generaci√≥n de contenido
‚îÇ       ‚îú‚îÄ‚îÄ email.py
‚îÇ       ‚îú‚îÄ‚îÄ historia.py
‚îÇ       ‚îú‚îÄ‚îÄ reel_cta.py
‚îÇ       ‚îî‚îÄ‚îÄ reel_lead_magnet.py
‚îÇ
‚îú‚îÄ‚îÄ dashboard/                  ‚Üê Interfaz visual Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  ‚Üê App principal con tabs de m√©tricas
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                ‚Üê Helpers de visualizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îî‚îÄ‚îÄ schema.sql              ‚Üê Definici√≥n de tablas, √≠ndices y funciones SQL
‚îÇ
‚îú‚îÄ‚îÄ documents_to_index/         ‚Üê Documentos de prueba (transcripciones Novotalks)
‚îÇ   ‚îú‚îÄ‚îÄ agustin.md
‚îÇ   ‚îú‚îÄ‚îÄ alex.md
‚îÇ   ‚îú‚îÄ‚îÄ andres.md
‚îÇ   ‚îú‚îÄ‚îÄ cristobal.md
‚îÇ   ‚îî‚îÄ‚îÄ lucas.md
‚îÇ
‚îú‚îÄ‚îÄ tests/poc/
‚îÇ   ‚îú‚îÄ‚îÄ test_token_tracker.py
‚îÇ   ‚îî‚îÄ‚îÄ test_cost_calculator.py
‚îÇ
‚îú‚îÄ‚îÄ logs/                       ‚Üê Generado autom√°ticamente al correr el POC
‚îÇ   ‚îú‚îÄ‚îÄ ingesta_log.csv
‚îÇ   ‚îú‚îÄ‚îÄ busqueda_log.csv
‚îÇ   ‚îú‚îÄ‚îÄ generacion_log.csv
‚îÇ   ‚îî‚îÄ‚îÄ poc_execution.log
‚îÇ
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## Flujo de datos de punta a punta

### Ingesta de un documento

Cuando llam√°s a `ingest_file("agustin.md")`, ocurre lo siguiente:

**1. Deduplicaci√≥n**
Se calcula un hash SHA-256 del contenido crudo. Si ya existe un documento con ese hash en Postgres (`metadata->>'content_hash'`), se saltea. Esto permite re-ejecutar el script sin duplicar datos.

**2. Parseo de frontmatter**
Se extrae el bloque YAML entre `---`. Si no hay frontmatter, el doc se procesa igual usando el nombre de archivo como t√≠tulo.

**3. Extracci√≥n heur√≠stica de entidades (costo $0)**
Sin llamar a ning√∫n LLM, se detectan:
- **Personas**: regex sobre el bloque "Participantes:" y patrones `Nombre Apellido:` en el texto.
- **Empresas/herramientas**: palabras con may√∫scula inicial que aparecen ‚â•2 veces en el documento.
- **Segmentos temporales**: l√≠neas con `[MM:SS]` usadas como l√≠mites de chunk.
- **Citas**: bloques `> "texto"` del formato markdown.

Todo esto se serializa como JSON y se guarda en `documents.metadata`, incluyendo el campo `graphiti_ready_context` (un string descriptivo listo para Graphiti).

**4. Chunking**
Si el documento tiene segmentos temporales detectados, se divide respet√°ndolos como fronteras naturales. Cada secci√≥n tem√°tica `[12:30] - Ventas por Relaci√≥n` se convierte en su propio chunk. Si no hay timestamps, se usa el `SemanticChunker` con overlap configurable.

**5. Embedding**
Se llama a `embedder.generate_embeddings_batch(chunks)` usando el singleton `get_embedder()`. Los tokens usados se registran en el `TokenTracker`.

**6. Persistencia en Postgres**
Se insertan:
- Un registro en `documents` con el contenido completo y toda la metadata.
- N registros en `chunks`, cada uno con su vector de embedding y su propia metadata (nombre del doc padre, √≠ndice, t√≠tulo del segmento).

**7. Ingesta en Graphiti (opcional)**
Si no se pas√≥ `--skip-graphiti`, se llama a `GraphClient.add_episode()` con el contenido y el `graphiti_ready_context` como `source_description`. Graphiti procesa el texto, extrae nodos (entidades) y aristas (relaciones) y los guarda en Neo4j.

---

### B√∫squeda

Hay tres modos:

**Vector search** (`vector_search_tool`):
1. Genera embedding de la query.
2. Ejecuta `SELECT ... ORDER BY embedding <=> $1 LIMIT $2` contra la tabla `chunks`.
3. Retorna los N chunks m√°s similares con sus scores.

**Graph search** (`graph_search_tool`):
1. Llama a `GraphClient.search(query)`.
2. Graphiti ejecuta internamente un pipeline que combina b√∫squeda sem√°ntica en Neo4j con razonamiento sobre las relaciones del grafo.
3. Retorna resultados como strings descriptivos de hechos y relaciones.

**Hybrid search** (`hybrid_search_tool`):
1. Genera embedding de la query.
2. Llama a la funci√≥n SQL `hybrid_search()` definida en `schema.sql`.
3. Esta funci√≥n combina los rankings de similitud coseno (vector) y `ts_rank` (full-text) mediante **Reciprocal Rank Fusion (RRF)**.
4. RRF fusiona los dos rankings con la f√≥rmula `1/(k + rank)` donde `k=60` por defecto, d√°ndole peso configurable a cada se√±al.

---

### Hidrataci√≥n del grafo (Fase 2)

`poc/hydrate_graph.py` orquesta la migraci√≥n:

1. Llama a `get_documents_missing_from_graph()` ‚Äî consulta Postgres filtrando por `metadata->>'graph_ingested' IS NOT TRUE`. Gracias al √≠ndice parcial en el schema, esta query es eficiente incluso con miles de documentos.
2. Por cada documento, extrae el `graphiti_ready_context` pre-calculado y lo inyecta como `source_description` en `GraphClient.add_episode()`.
3. Una vez procesado, llama a `mark_document_graph_ingested(doc_id)` que hace un `metadata || '{"graph_ingested": true}'::jsonb` ‚Äî actualizaci√≥n at√≥mica sin reescribir todo el JSONB.
4. Al final, muestra el costo total estimado y proyecci√≥n mensual con decisi√≥n GO/OPTIMIZE/STOP.

---

## Sistema de m√©tricas y costos

### TokenTracker (`poc/token_tracker.py`)

Singleton thread-safe (con `threading.Lock`) que vive durante toda la ejecuci√≥n. Cada operaci√≥n tiene un ciclo de vida:

```python
# Inicio de una operaci√≥n
tracker.start_operation("ingest_agustin_1234", "ingestion")

# Registro de uso (puede llamarse m√∫ltiples veces por operaci√≥n)
tracker.record_usage(op_id, tokens_in=450, tokens_out=0, model="text-embedding-3-small", detail_name="embedding")

# Fin y obtenci√≥n de m√©tricas acumuladas
metrics = tracker.end_operation(op_id)
# metrics.tokens_in, metrics.tokens_out, metrics.cost_usd, metrics.details
```

Si `tiktoken` est√° disponible, cuenta tokens con exactitud. Si no, usa la heur√≠stica `len(text) // 4`.

### CostCalculator (`poc/cost_calculator.py`)

Multiplica tokens por los precios definidos en `MODEL_PRICING` dentro de `poc/config.py`:

| Modelo | Input ($/1M tokens) | Output ($/1M tokens) |
|--------|---------------------|----------------------|
| `gpt-5-mini` | $0.080 | $0.320 |
| `gpt-4o-mini` | $0.150 | $0.600 |
| `gemini-1.5-flash` | $0.075 | $0.300 |
| `text-embedding-3-small` | $0.020 | ‚Äî |
| `text-embedding-004` | $0.025 | ‚Äî |

### Logs CSV (`logs/`)

Cada operaci√≥n escribe en uno de tres archivos CSV thread-safe:

**`ingesta_log.csv`** ‚Äî Una fila por documento ingestado:
`episodio_id, timestamp, nombre_archivo, longitud_palabras, chunks_creados, embeddings_tokens, entidades_detectadas, costo_total_usd, tiempo_seg`

**`busqueda_log.csv`** ‚Äî Una fila por b√∫squeda ejecutada:
`query_id, timestamp, query_texto, tipo_busqueda, tokens_embedding, tokens_llm_in, tokens_llm_out, costo_total_usd, resultados_retornados, latencia_ms`

**`generacion_log.csv`** ‚Äî Una fila por pieza de contenido generada:
`pieza_id, timestamp, formato, tema_base, tokens_contexto_in, tokens_prompt_in, tokens_out, modelo, provider, costo_usd, tiempo_seg, longitud_output_chars`

---

## Archivos clave en detalle

### `agent/db_utils.py`

Gestiona toda la interacci√≥n con PostgreSQL usando `asyncpg` con un pool de conexiones (min=2, max=10). Funciones principales:

- `DatabasePool.init_db()` ‚Äî Crea las extensiones y aplica `schema.sql` si la tabla no existe. Detecta autom√°ticamente si usar `vector(1536)` (OpenAI) o `vector(768)` (Gemini).
- `insert_document(title, source, content, metadata)` ‚Äî Inserta en la tabla `documents` y retorna el UUID.
- `insert_chunks(doc_id, chunks, embeddings, chunk_metas)` ‚Äî Inserta en batch en la tabla `chunks` usando `_fmt_vec()` para el formato correcto de pgvector.
- `document_exists_by_hash(hash)` ‚Äî Consulta `metadata->>'content_hash'` para deduplicaci√≥n.
- `get_documents_missing_from_graph(limit)` ‚Äî Para `hydrate_graph.py`. Usa el √≠ndice parcial en `graph_ingested`.
- `mark_document_graph_ingested(doc_id)` ‚Äî Actualiza el flag JSONB at√≥micamente.
- `vector_search(embedding, limit)` ‚Äî B√∫squeda cosine similarity.
- `hybrid_search(text, embedding, limit)` ‚Äî Llama a la funci√≥n SQL `hybrid_search()`.

### `agent/graph_utils.py`

Wrapper sobre `graphiti-core`. Se inicializa lazy (la primera vez que se llama a `get_client()`).

- Soporta OpenAI y Gemini como backend del LLM de Graphiti.
- `add_episode(content, source_reference, source_description)` ‚Äî El par√°metro `source_description` es el punto de entrada del contexto pre-calculado. Sin √©l, Graphiti gasta m√°s tokens intentando inferir el tipo y contenido del documento.
- `search(query)` ‚Äî B√∫squeda sem√°ntica + relacional en el grafo.
- Estima el costo de cada episodio usando un ratio de 30% output/input (basado en el comportamiento real de Graphiti, m√°s conservador que el 50% te√≥rico).

### `ingestion/embedder.py`

Generador de embeddings con patr√≥n singleton via `@lru_cache` en `get_embedder()`. Esto evita crear un nuevo cliente HTTP por cada b√∫squeda o ingesta.

- Para OpenAI: llama a `AsyncOpenAI.embeddings.create()` de forma nativa async.
- Para Gemini: `embed_content()` es sincr√≥nico, por eso se envuelve en `asyncio.to_thread()` para no bloquear el event loop.
- `generate_embeddings_batch(texts)` ‚Äî Procesa una lista de textos en un solo llamado API y retorna `(embeddings, total_tokens)`.

### `poc/config.py`

Configuraci√≥n central usando Pydantic v2 `BaseSettings`. Lee del `.env` autom√°ticamente. Usa un `@model_validator(mode="after")` para resolver los modelos por defecto de Gemini en el momento de construcci√≥n (no despu√©s), evitando mutaciones post-construcci√≥n que Pydantic v2 no permite.

### `sql/schema.sql`

Adem√°s de las tablas est√°ndar, define:

- **√çndice HNSW** en `chunks.embedding` para b√∫squeda aproximada de vecinos eficiente.
- **√çndice GIN** en `content_tsvector` (columna generada) para full-text search.
- **√çndice parcial** en `metadata->>'graph_ingested'` ‚Äî solo indexa los documentos *no* hidratados, lo que lo mantiene peque√±o y r√°pido.
- **√çndice en `content_hash`** para deduplicaci√≥n O(log n).
- **Funci√≥n `hybrid_search()`** ‚Äî implementa RRF en PL/pgSQL directamente en la base de datos.
- **Vista `v_document_summary`** ‚Äî resumen por documento: cu√°ntos chunks tiene, si fue hidratado al grafo, tokens totales.

---

## Dashboard

```bash
streamlit run dashboard/app.py
```

El dashboard tiene **seis tabs** principales:

1.  **üì• Ingesta**: Trigger para procesar nuevos documentos. Opci√≥n `--skip-graphiti` para iteraci√≥n r√°pida.
2.  **üß† Knowledge Base**: Visor de la base de datos. Muestra todos los documentos ingestados, conteo de chunks y metadata extra√≠da. Permite filtrar por nombre.
3.  **üîç B√∫squeda**: Interfaz para probar Vector, Graph y Hybrid search. Incluye un **Debug Mode** para inspeccionar el JSON crudo y los scores RRF.
4.  **‚ú® Generaci√≥n**: Templates predefinidos (Email, Historia, Reel) y un nuevo **Modo Custom** para experimentar con prompts libres.
5.  **üìä Analytics**: M√©tricas de costo total y gr√°ficos de evoluci√≥n temporal por tipo de operaci√≥n (Ingesta, B√∫squeda, Generaci√≥n).
6.  **üìà Proyecciones**: Calculadora de ROI y estimaci√≥n de costos mensuales seg√∫n volumen esperado.

**Acciones de la Sidebar**:
- **üóëÔ∏è Clear Logs & DB**: Limpieza total para reiniciar pruebas.
- **üíß Re-hydrate Graph**: Forza la ingesta de documentos pendientes desde Postgres hacia Neo4j sin re-procesar embeddings.

---

## Criterios de √©xito

El POC usa estos umbrales para decidir si escalar a producci√≥n:

| Decisi√≥n | Costo por episodio | Costo mensual | Costo anual |
|----------|--------------------|---------------|-------------|
| ‚úÖ **GO** | < $0.40 | < $100 | < $1,500 |
| ‚ö†Ô∏è **OPTIMIZE** | $0.40 ‚Äì $0.70 | $100 ‚Äì $200 | $1,500 ‚Äì $3,000 |
| üõë **STOP** | > $0.70 | > $200 | > $3,000 |

Un "episodio" es la ingesta de un documento completo a Graphiti (extracci√≥n de entidades + relaciones). La Fase 1 (solo embeddings) tiene un costo √≥rdenes de magnitud menor y no entra en esta evaluaci√≥n.

---

## Preguntas frecuentes

**¬øPor qu√© Postgres y Neo4j en lugar de solo uno?**
Postgres con pgvector es excelente para b√∫squeda sem√°ntica pero no modela relaciones entre entidades. Neo4j/Graphiti es excelente para razonar sobre relaciones ("¬øqui√©n invirti√≥ en qu√© empresa?", "¬øqu√© personas comparten metodolog√≠as?") pero m√°s lento y costoso. La arquitectura h√≠brida toma lo mejor de cada uno.

**¬øQu√© es Graphiti exactamente?**
`graphiti-core` es una librer√≠a open source que toma texto libre y autom√°ticamente extrae entidades (personas, empresas, conceptos) y las relaciones entre ellas, guard√°ndolas como nodos y aristas en Neo4j. Internamente llama al LLM configurado (OpenAI o Gemini) con varios prompts encadenados.

**¬øPor qu√© se pre-calculan los metadatos en Fase 1 si Graphiti los va a extraer igual en Fase 2?**
Porque el `source_description` que se le pasa a `add_episode()` gu√≠a el LLM de Graphiti. Sin √©l, Graphiti necesita inferir el tipo de documento, sus participantes y sus temas desde cero ‚Äî lo que consume prompts completos. Con el contexto pre-calculado, el LLM puede enfocarse directamente en extraer relaciones en lugar de descubrir informaci√≥n que ya tenemos. El ahorro estimado es 20‚Äì30% en tokens por episodio.

**¬øQu√© pasa si la hidrataci√≥n se corta a mitad?**
`hydrate_graph.py` es reanudable. Cada documento procesado exitosamente recibe el flag `metadata->>'graph_ingested': true` en Postgres. La pr√≥xima ejecuci√≥n consulta solo los documentos sin ese flag usando el √≠ndice parcial, as√≠ que no reprocesa nada.

**¬øC√≥mo agrego m√°s documentos?**
Ponelos en `documents_to_index/` y corr√©s el pipeline de nuevo. La deduplicaci√≥n por hash SHA-256 garantiza que los documentos que ya est√°n procesados no se vuelven a ingestar.

**¬øPuedo usar solo Gemini?**
S√≠. En el `.env` poner `LLM_PROVIDER=gemini` y `GEMINI_API_KEY=...`. El sistema autom√°ticamente usa `gemini-1.5-flash` para el LLM y `text-embedding-004` (768 dimensiones) para embeddings. El schema se ajusta al ejecutar `init_db()`.

**¬øC√≥mo corro los tests?**
```bash
pytest tests/poc/
```
Hay tests para `TokenTracker` (thread-safety, acumulaci√≥n de costos) y `CostCalculator` (precios por modelo).